{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Image classification is the process of taking an input (like a picture) and outputting a class (like “cat”) or a probability that the input is a particular class (“there’s a 90% probability that this input is a cat”). You can look at a picture and know that you’re looking at a terrible shot of your own face, but how can a computer learn to do that? With a convolutional neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Goals\n",
    "We would like you to establish a neural network involving advanced DNN modules (i.e. convolution layers, RELU, pooling and fully connection layers and etc.) to distinguish the specific category of an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Packages\n",
    "Let's first import the necessary packages,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "from torch import Tensor\n",
    "import os\n",
    "import numpy as np\n",
    "import os.path\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data\n",
    "from ipywidgets import IntProgress\n",
    "from torchvision import transforms\n",
    "import logging \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## GPU Device Configuration\n",
    "Use the torch.device() and torch.cuda.is_available() functions to make sure you can use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print (device)\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Configuration\n",
    "### hyperparameters\n",
    "We then set up the hyper parameters.\n",
    "we need to define several hyper parameters for our model:\n",
    "1. learning rate\n",
    "2. batch size when training\n",
    "3. batch size when testing\n",
    "4. number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_train_size = 20\n",
    "batch_test_size = 20\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory if it does not exist\n",
    "you can use os.path.exists() to check whether it exists and using os.makedirs to create a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "###  Image processing\n",
    "Then, we define an image preprocessing object that our dataloader will use to preprocess our data. We use the pytorch API to preform the data processing.\n",
    "1. Use transforms.Compose()\n",
    "2. Use .RandomHorizontalFlip()\n",
    "3. You add any extra transforms you like.\n",
    "4. Create this transform for both the train set and test set. Note that for the test, we do not require any transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### We then download and prepare the data with the transforms defined above:\n",
    "1. Use command torchvision.datasets.CIFAR10() with root, train, download and transform positional arguments.\n",
    "2. Use the same command to create both train split and test split.\n",
    "3. Use torch.utils.data.DataLoader() to create the data loader based on the data we have.\n",
    "3. Use this command for both the training split data loader and test split data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = dset.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "train_loader = data.DataLoader(dataset=train_set, batch_size=batch_train_size, shuffle=True)\n",
    "test_set = dset.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = data.DataLoader(dataset=test_set, batch_size=batch_test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Inception Module with dimension reductions\n",
    "1. Create a python class called Inception which inherits nn.module\n",
    "\n",
    "2. Create a init function to init this python class\n",
    "    1. Require in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x and pool_planes 7 arguments.\n",
    "    \n",
    "    2. There are 4 Sequential blocks: b1,b2,b3,b4\n",
    "    \n",
    "    3. b1 is a block that consists of 2D convolution, a 2D batch normalization layer and a ReLU activation function\n",
    "    \n",
    "    4. b2 is a block that consists of two 2D convolutions, two 2D batch normalization layers and two ReLU activation functions\n",
    "    \n",
    "    5. b3 is a block that consists of two 2D convolutions, two 2D batch normalization layers and two ReLU activation functions\n",
    "    \n",
    "    6. b4 is a block consists of a Maxpooling layer, a 2D convolution, a 2D batch normalization layer and a ReLU activation function\n",
    "    \n",
    "3. Create the forward function: the forward function will forward the input function though every block and return the concatenation of all the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_1_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "       # Block 2: 1x1 convolution followed by 3x3 convolution\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_3_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_3_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_3_x),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Block 3: 1x1 convolution followed by 5x5 convolution\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_5_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_5_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Block 4: MaxPooling followed by 1x1 convolution\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply all blocks to the input\n",
    "            out1 = self.b1(x)\n",
    "            out2 = self.b2(x)\n",
    "            out3 = self.b3(x)\n",
    "            out4 = self.b4(x)\n",
    "        # Concatenate along the channel dimension\n",
    "            return torch.cat([out1, out2, out3, out4], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### GoogLeNet Module: the structure is in the lab manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):  # Default for CIFAR-10\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        \n",
    "        # Initial convolutional layers\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Inception Blocks\n",
    "        self.inception3a = InceptionBlock(64, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        # Final average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling to output 1x1\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.pre_layers(x)\n",
    "        \n",
    "        # Inception blocks with intermediate max pooling\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        # Final layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten for the fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create the network and send it to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogLeNet(\n",
      "  (pre_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (inception3a): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (inception3b): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception4a): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (inception4b): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (inception4c): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (inception4d): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (inception4e): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception5a): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (inception5b): InceptionBlock(\n",
      "    (b1): Sequential(\n",
      "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (b2): Sequential(\n",
      "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b3): Sequential(\n",
      "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (b4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10 \n",
    "model = GoogLeNet(num_classes=num_classes)\n",
    "\n",
    "# Move the model to the target device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the model to confirm it's correctly initialized\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we create:\n",
    " 1. An optimizer  (we use adam optimzer here)\n",
    " 2. A Criterion (CrossEntropy) function\n",
    " 3. A Scheduler which decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.5\n",
    "milestones = [50, 100]\n",
    "\n",
    "# Optimizer: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Criterion: Cross-Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scheduler: MultiStepLR\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Training\n",
    "Then, we are going to train our Network\n",
    "\n",
    "1. Set our network to the training mode.\n",
    "2. Initialize the train loss, total data size, and number corrected predictions. \n",
    "3. For each data in the training split\n",
    "    1. Put the data to the correct devices using .to()\n",
    "    2. Reset the gradient of the optimzier.\n",
    "    3. Feed the data forward to the google net\n",
    "    4. Use the criterion function to compute the loss term\n",
    "    5. Backpropagate the loss\n",
    "    6. Update the network parameters using the optimizier\n",
    "    7. Accumulate the training loss\n",
    "    8. Find the prediction. hint: using torch.max()\n",
    "    9. Increment the total_data size\n",
    "    10. Increment the corrected prediction\n",
    "    11. Print log\n",
    "    \n",
    "-----\n",
    "##  Testing\n",
    "Then, we are going to test our module\n",
    "\n",
    "1. Set our network to the test model.\n",
    "2. Initialize the test loss, total data size, and number corrected predictions. \n",
    "3. For each data in the testing split, we warp it using torch.no_grad()\n",
    "    1. Put the data to the correct devices using .to()\n",
    "    2. Feed the data forward to the google net\n",
    "    3. Use the criterion function to compute the loss term\n",
    "    4. Accumulate the testing loss\n",
    "    5. Find the prediciton. hint: using torch.max()\n",
    "    6. Increment the data size\n",
    "    7. Increment the corrected prediction\n",
    "    8. Print log\n",
    "\n",
    "-----\n",
    "##  Epochs:\n",
    "For each epoch:\n",
    "1. Train the model\n",
    "2. Step the scheduler\n",
    "3. Test our model\n",
    "4. Update the accuracies\n",
    "5. Save the module at the end and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Loss: 1.5740, Training Accuracy: 43.03%\n",
      "Test Loss: 1.2536, Test Accuracy: 53.46%\n",
      "New best model saved with accuracy: 53.46%\n",
      "Epoch 2/50\n",
      "Training Loss: 1.1086, Training Accuracy: 61.38%\n",
      "Test Loss: 1.0397, Test Accuracy: 63.64%\n",
      "New best model saved with accuracy: 63.64%\n",
      "Epoch 3/50\n",
      "Training Loss: 0.9066, Training Accuracy: 68.99%\n",
      "Test Loss: 0.8594, Test Accuracy: 70.39%\n",
      "New best model saved with accuracy: 70.39%\n",
      "Epoch 4/50\n",
      "Training Loss: 0.7837, Training Accuracy: 73.67%\n",
      "Test Loss: 0.8642, Test Accuracy: 71.54%\n",
      "New best model saved with accuracy: 71.54%\n",
      "Epoch 5/50\n",
      "Training Loss: 0.6917, Training Accuracy: 76.67%\n",
      "Test Loss: 0.7581, Test Accuracy: 73.73%\n",
      "New best model saved with accuracy: 73.73%\n",
      "Epoch 6/50\n",
      "Training Loss: 0.6120, Training Accuracy: 79.35%\n",
      "Test Loss: 0.6884, Test Accuracy: 76.43%\n",
      "New best model saved with accuracy: 76.43%\n",
      "Epoch 7/50\n",
      "Training Loss: 0.5613, Training Accuracy: 81.06%\n",
      "Test Loss: 0.6420, Test Accuracy: 78.57%\n",
      "New best model saved with accuracy: 78.57%\n",
      "Epoch 8/50\n",
      "Training Loss: 0.5052, Training Accuracy: 82.97%\n",
      "Test Loss: 0.5924, Test Accuracy: 79.91%\n",
      "New best model saved with accuracy: 79.91%\n",
      "Epoch 9/50\n",
      "Training Loss: 0.4615, Training Accuracy: 84.35%\n",
      "Test Loss: 0.5882, Test Accuracy: 80.20%\n",
      "New best model saved with accuracy: 80.20%\n",
      "Epoch 10/50\n",
      "Training Loss: 0.4248, Training Accuracy: 85.63%\n",
      "Test Loss: 0.6393, Test Accuracy: 79.34%\n",
      "Epoch 11/50\n",
      "Training Loss: 0.3945, Training Accuracy: 86.64%\n",
      "Test Loss: 0.6299, Test Accuracy: 79.41%\n",
      "Epoch 12/50\n",
      "Training Loss: 0.3539, Training Accuracy: 88.04%\n",
      "Test Loss: 0.6648, Test Accuracy: 79.00%\n",
      "Epoch 13/50\n",
      "Training Loss: 0.3255, Training Accuracy: 89.09%\n",
      "Test Loss: 0.5476, Test Accuracy: 81.76%\n",
      "New best model saved with accuracy: 81.76%\n",
      "Epoch 14/50\n",
      "Training Loss: 0.3014, Training Accuracy: 89.59%\n",
      "Test Loss: 0.6251, Test Accuracy: 80.10%\n",
      "Epoch 15/50\n",
      "Training Loss: 0.2790, Training Accuracy: 90.57%\n",
      "Test Loss: 1.6609, Test Accuracy: 77.52%\n",
      "Epoch 16/50\n",
      "Training Loss: 0.2543, Training Accuracy: 91.33%\n",
      "Test Loss: 0.5771, Test Accuracy: 82.80%\n",
      "New best model saved with accuracy: 82.80%\n",
      "Epoch 17/50\n",
      "Training Loss: 0.2389, Training Accuracy: 91.98%\n",
      "Test Loss: 0.6213, Test Accuracy: 81.80%\n",
      "Epoch 18/50\n",
      "Training Loss: 0.2284, Training Accuracy: 92.27%\n",
      "Test Loss: 0.6143, Test Accuracy: 81.78%\n",
      "Epoch 19/50\n",
      "Training Loss: 0.2128, Training Accuracy: 92.91%\n",
      "Test Loss: 0.6027, Test Accuracy: 82.58%\n",
      "Epoch 20/50\n",
      "Training Loss: 0.1915, Training Accuracy: 93.50%\n",
      "Test Loss: 0.6478, Test Accuracy: 81.75%\n",
      "Epoch 21/50\n",
      "Training Loss: 0.1782, Training Accuracy: 94.04%\n",
      "Test Loss: 0.6192, Test Accuracy: 82.53%\n",
      "Epoch 22/50\n",
      "Training Loss: 0.1688, Training Accuracy: 94.28%\n",
      "Test Loss: 0.6520, Test Accuracy: 81.98%\n",
      "Epoch 23/50\n",
      "Training Loss: 0.1564, Training Accuracy: 94.80%\n",
      "Test Loss: 0.6427, Test Accuracy: 81.90%\n",
      "Epoch 24/50\n",
      "Training Loss: 0.1488, Training Accuracy: 94.92%\n",
      "Test Loss: 0.6339, Test Accuracy: 82.94%\n",
      "New best model saved with accuracy: 82.94%\n",
      "Epoch 25/50\n",
      "Training Loss: 0.1477, Training Accuracy: 94.98%\n",
      "Test Loss: 0.6719, Test Accuracy: 82.16%\n",
      "Epoch 26/50\n",
      "Training Loss: 0.1280, Training Accuracy: 95.64%\n",
      "Test Loss: 0.7349, Test Accuracy: 81.41%\n",
      "Epoch 27/50\n",
      "Training Loss: 0.1257, Training Accuracy: 95.80%\n",
      "Test Loss: 0.7282, Test Accuracy: 81.78%\n",
      "Epoch 28/50\n",
      "Training Loss: 0.1210, Training Accuracy: 96.03%\n",
      "Test Loss: 0.6270, Test Accuracy: 83.20%\n",
      "New best model saved with accuracy: 83.20%\n",
      "Epoch 29/50\n",
      "Training Loss: 0.1129, Training Accuracy: 96.12%\n",
      "Test Loss: 0.7104, Test Accuracy: 83.49%\n",
      "New best model saved with accuracy: 83.49%\n",
      "Epoch 30/50\n",
      "Training Loss: 0.1115, Training Accuracy: 96.24%\n",
      "Test Loss: 0.8530, Test Accuracy: 82.20%\n",
      "Epoch 31/50\n",
      "Training Loss: 0.1071, Training Accuracy: 96.38%\n",
      "Test Loss: 0.6442, Test Accuracy: 83.01%\n",
      "Epoch 32/50\n",
      "Training Loss: 0.1009, Training Accuracy: 96.65%\n",
      "Test Loss: 0.6609, Test Accuracy: 83.21%\n",
      "Epoch 33/50\n",
      "Training Loss: 0.0987, Training Accuracy: 96.66%\n",
      "Test Loss: 0.7286, Test Accuracy: 82.41%\n",
      "Epoch 34/50\n",
      "Training Loss: 0.0950, Training Accuracy: 96.82%\n",
      "Test Loss: 0.7705, Test Accuracy: 82.97%\n",
      "Epoch 35/50\n",
      "Training Loss: 0.1006, Training Accuracy: 96.54%\n",
      "Test Loss: 1.0248, Test Accuracy: 82.12%\n",
      "Epoch 36/50\n",
      "Training Loss: 0.0849, Training Accuracy: 97.22%\n",
      "Test Loss: 0.8083, Test Accuracy: 82.21%\n",
      "Epoch 37/50\n",
      "Training Loss: 0.0840, Training Accuracy: 97.14%\n",
      "Test Loss: 0.8241, Test Accuracy: 82.79%\n",
      "Epoch 38/50\n",
      "Training Loss: 0.0869, Training Accuracy: 97.05%\n",
      "Test Loss: 0.6865, Test Accuracy: 83.24%\n",
      "Epoch 39/50\n",
      "Training Loss: 0.0745, Training Accuracy: 97.48%\n",
      "Test Loss: 0.8732, Test Accuracy: 82.25%\n",
      "Epoch 40/50\n",
      "Training Loss: 0.0779, Training Accuracy: 97.35%\n",
      "Test Loss: 3.0700, Test Accuracy: 79.06%\n",
      "Epoch 41/50\n",
      "Training Loss: 0.0815, Training Accuracy: 97.22%\n",
      "Test Loss: 0.8524, Test Accuracy: 82.80%\n",
      "Epoch 42/50\n",
      "Training Loss: 0.0709, Training Accuracy: 97.62%\n",
      "Test Loss: 0.7098, Test Accuracy: 83.90%\n",
      "New best model saved with accuracy: 83.90%\n",
      "Epoch 43/50\n",
      "Training Loss: 0.0711, Training Accuracy: 97.65%\n",
      "Test Loss: 0.7734, Test Accuracy: 82.91%\n",
      "Epoch 44/50\n",
      "Training Loss: 0.0714, Training Accuracy: 97.61%\n",
      "Test Loss: 0.7485, Test Accuracy: 83.67%\n",
      "Epoch 45/50\n",
      "Training Loss: 0.0694, Training Accuracy: 97.64%\n",
      "Test Loss: 0.7349, Test Accuracy: 83.58%\n",
      "Epoch 46/50\n",
      "Training Loss: 0.0656, Training Accuracy: 97.79%\n",
      "Test Loss: 0.7248, Test Accuracy: 83.78%\n",
      "Epoch 47/50\n",
      "Training Loss: 0.0676, Training Accuracy: 97.76%\n",
      "Test Loss: 0.7880, Test Accuracy: 83.31%\n",
      "Epoch 48/50\n",
      "Training Loss: 0.0616, Training Accuracy: 97.92%\n",
      "Test Loss: 0.8976, Test Accuracy: 82.16%\n",
      "Epoch 49/50\n",
      "Training Loss: 0.0627, Training Accuracy: 97.86%\n",
      "Test Loss: 1.5224, Test Accuracy: 81.72%\n",
      "Epoch 50/50\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "milestones = [50, 100]\n",
    "best_accuracy = 0.0  # To track the best test accuracy\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_train_size = 20\n",
    "batch_test_size = 20\n",
    "num_epochs = 50\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "epochs=list(range(1, 1+num_epochs))\n",
    "\n",
    "# Optimizer, criterion, and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# Training and testing loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    # Training \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for inputs, labels in train_loader: \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate training loss and predictions\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy.append((correct_train / total_train) * 100)\n",
    "    value1=(correct_train / total_train) * 100\n",
    "    train_losses.append(train_loss / len(train_loader))  # Average train loss\n",
    "\n",
    "    print(f\"Training Loss: {train_loss/len(train_loader):.4f}, Training Accuracy: {value1:.2f}%\")\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    total_test = 0\n",
    "    correct_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:  # Assuming test_loader is defined\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Accumulate test loss and calculate predictions\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy.append( (correct_test / total_test) * 100)\n",
    "    value=(correct_test / total_test) * 100\n",
    "    test_losses.append(test_loss / len(test_loader)) \n",
    "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {value:.2f}%\")\n",
    "    \n",
    "    # Update best accuracy and save model\n",
    "    if value > best_accuracy:\n",
    "        best_accuracy = value\n",
    "        torch.save(model.state_dict(), \"best_googlenet_model.pth\")\n",
    "        print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"Training complete. Best Test Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, test_accuracy, label='Test Accuracy', marker='x')\n",
    "plt.title('Train and Test Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, test_losses, label='Test Loss', marker='x')\n",
    "plt.title('Loss Convergence Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
