{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits #import the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pixel_0  pixel_1  pixel_2  pixel_3  pixel_4  pixel_5  pixel_6  pixel_7  \\\n",
      "0      0.0      0.0      5.0     13.0      9.0      1.0      0.0      0.0   \n",
      "1      0.0      0.0      0.0     12.0     13.0      5.0      0.0      0.0   \n",
      "2      0.0      0.0      0.0      4.0     15.0     12.0      0.0      0.0   \n",
      "3      0.0      0.0      7.0     15.0     13.0      1.0      0.0      0.0   \n",
      "4      0.0      0.0      0.0      1.0     11.0      0.0      0.0      0.0   \n",
      "\n",
      "   pixel_8  pixel_9  ...  pixel_55  pixel_56  pixel_57  pixel_58  pixel_59  \\\n",
      "0      0.0      0.0  ...       0.0       0.0       0.0       6.0      13.0   \n",
      "1      0.0      0.0  ...       0.0       0.0       0.0       0.0      11.0   \n",
      "2      0.0      0.0  ...       0.0       0.0       0.0       0.0       3.0   \n",
      "3      0.0      8.0  ...       0.0       0.0       0.0       7.0      13.0   \n",
      "4      0.0      0.0  ...       0.0       0.0       0.0       0.0       2.0   \n",
      "\n",
      "   pixel_60  pixel_61  pixel_62  pixel_63  target  \n",
      "0      10.0       0.0       0.0       0.0       0  \n",
      "1      16.0      10.0       0.0       0.0       1  \n",
      "2      11.0      16.0       9.0       0.0       2  \n",
      "3      13.0       9.0       0.0       0.0       3  \n",
      "4      16.0       4.0       0.0       0.0       4  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "\n",
    "#Create a DataFrame\n",
    "digits_data = pd.DataFrame(digits.data, columns=[f'pixel_{i}' for i in range(digits.data.shape[1])])\n",
    "digits_data['target'] = digits.target  # Add target labels for easier inspection\n",
    "\n",
    "# Display the first few rows for visualization purposes\n",
    "print(digits_data.head())\n",
    "\n",
    "y_one_hot =[]\n",
    "for (target) in Y:\n",
    "    one_hot=np.zeros(10)\n",
    "    one_hot[target] = 1\n",
    "    y_one_hot.append(one_hot)\n",
    "y_one_hot = np.array(y_one_hot)\n",
    "\n",
    "#Divide into train and test data\n",
    "X_train2, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train2)\n",
    "# Transform the test data using the parameters learned from the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Set the network structure as [64, 30, 10] -> 64 for the input layer, 30 for the hidden layer, and 10 for the output layer\n",
    "input_size = 64     # Input layer size\n",
    "hidden_size = 30    # Hidden layer size\n",
    "output_size = 10    # Output layer size\n",
    "\n",
    "# #One hot encoded the target\n",
    "# y_train_encoded = np.zeros((y_train.size, output_size))\n",
    "# y_train_encoded[np.arange(y_train.size), y_train] = 1\n",
    "# print (y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function: Sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function: ReLu\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function: tanH\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedForward function\n",
    "#For every X, multiply by the corresponding weight and add the bias\n",
    "#Pass the previous result into the activation function\n",
    "#Pass previous result as input into the next layer\n",
    "\n",
    "def FeedForward_sigmoid (X_train1, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output):\n",
    "    hidden_layer_input = np.dot(X_train1, weights_input_hidden) + biases_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_output\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    return hidden_layer_input, output_layer_input, hidden_layer_output, output_layer_output\n",
    "     \n",
    "def FeedForward_relu (X_train1, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output):\n",
    "    hidden_layer_input = np.dot(X_train1, weights_input_hidden) + biases_hidden\n",
    "    hidden_layer_output = relu(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_output\n",
    "    output_layer_output = relu(output_layer_input)\n",
    "    return hidden_layer_input, output_layer_input, hidden_layer_output, output_layer_output\n",
    "\n",
    "def FeedForward_tanh (X_train1, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output):\n",
    "    hidden_layer_input = np.dot(X_train1, weights_input_hidden) + biases_hidden\n",
    "    hidden_layer_output = tanh(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_output\n",
    "    output_layer_output = tanh(output_layer_input)\n",
    "    return hidden_layer_input, output_layer_input, hidden_layer_output, output_layer_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation_sigmoid(learning_rate, epochs, batch_size, X_train, y_train, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, activation=\"sigmoid\"):\n",
    "    #Use Mini Batch Gradient descent for better results due to the size of the dataset\n",
    "    num_samples = X_train.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data at the start of each epoch\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        # Iterate over mini-batches\n",
    "        for start_id in range(0, num_samples, batch_size):\n",
    "            end_id = min(start_id + batch_size, num_samples)\n",
    "            X_batch = X_train[start_id:end_id]\n",
    "            y_batch = y_train[start_id:end_id]\n",
    "            \n",
    "            # Feedforward pass\n",
    "            if (activation==\"sigmoid\"):\n",
    "                hidden_layer_input, output_layer_input, hidden_layer_output, output_layer_output = FeedForward_sigmoid(X_batch, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output)\n",
    "                error_output = (output_layer_output - y_batch) * sigmoid_derivative(output_layer_input)\n",
    "                error_hidden_layer = np.dot(error_output, weights_hidden_output.T) * sigmoid_derivative(hidden_layer_input)\n",
    "            elif (activation==\"tanh\"):\n",
    "                hidden_layer_input, output_layer_input, hidden_layer_output, output_layer_output = FeedForward_tanh(X_batch, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output)\n",
    "                error_output = (output_layer_output - y_batch) * tanh_derivative(output_layer_input)\n",
    "                error_hidden_layer = np.dot(error_output, weights_hidden_output.T) * tanh_derivative(hidden_layer_input)\n",
    "            else:\n",
    "                hidden_layer_input, output_layer_input, hidden_layer_output, output_layer_output = FeedForward_relu(X_batch, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output)\n",
    "                error_output = (output_layer_output - y_batch) * relu_derivative(output_layer_input)\n",
    "                error_hidden_layer = np.dot(error_output, weights_hidden_output.T) * relu_derivative(hidden_layer_input)\n",
    "                \n",
    "            # Calculate loss (MSE for the batch)\n",
    "            loss = np.mean((y_batch - output_layer_output) ** 2)\n",
    "            \n",
    "            # Backpropagation\n",
    "            # Output layer error\n",
    "            #error_output = (output_layer_output - y_batch) * sigmoid_derivative(output_layer_input)\n",
    "            gradient_weights_hidden_output = np.dot(hidden_layer_output.T, error_output) / batch_size\n",
    "            gradient_biases_output = np.mean(error_output, axis=0)\n",
    "\n",
    "            # Hidden layer error\n",
    "            #error_hidden_layer = np.dot(error_output, weights_hidden_output.T) * sigmoid_derivative(hidden_layer_input)\n",
    "            gradient_weights_input_hidden = np.dot(X_batch.T, error_hidden_layer) / batch_size\n",
    "            gradient_biases_hidden = np.mean(error_hidden_layer, axis=0)\n",
    "\n",
    "            # Update weights and biases\n",
    "            weights_hidden_output -= learning_rate * gradient_weights_hidden_output\n",
    "            biases_output -= learning_rate * gradient_biases_output\n",
    "            weights_input_hidden -= learning_rate * gradient_weights_input_hidden\n",
    "            biases_hidden -= learning_rate * gradient_biases_hidden\n",
    "        \n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    return weights_input_hidden, biases_hidden, weights_hidden_output, biases_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(X, y, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, activation1):\n",
    "    # Feedforward pass on the test data\n",
    "    if (activation1==\"sigmoid\"):\n",
    "        _, _, _, output_layer_output = FeedForward_sigmoid(X, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output)\n",
    "    elif (activation1==\"tanh\"):\n",
    "        _, _, _, output_layer_output = FeedForward_tanh(X, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output)\n",
    "    else:\n",
    "        _, _, _, output_layer_output = FeedForward_relu(X, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output)\n",
    "    # Convert output layer output to predicted class labels\n",
    "    y_pred = np.argmax(output_layer_output, axis=1)\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_true) * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7132\n",
      "Epoch 100, Loss: 0.4344\n",
      "Epoch 200, Loss: 0.0786\n",
      "Epoch 300, Loss: 0.0793\n",
      "Epoch 400, Loss: 0.0724\n",
      "Epoch 500, Loss: 0.0684\n",
      "Epoch 600, Loss: 0.0594\n",
      "Epoch 700, Loss: 0.0654\n",
      "Epoch 800, Loss: 0.0553\n",
      "Epoch 900, Loss: 0.0549\n",
      "Epoch 1000, Loss: 0.0480\n",
      "Epoch 1100, Loss: 0.0482\n",
      "Epoch 1200, Loss: 0.0246\n",
      "Epoch 1300, Loss: 0.0255\n",
      "Epoch 1400, Loss: 0.0331\n",
      "Epoch 1500, Loss: 0.0149\n",
      "Epoch 1600, Loss: 0.0256\n",
      "Epoch 1700, Loss: 0.0274\n",
      "Epoch 1800, Loss: 0.0172\n",
      "Epoch 1900, Loss: 0.0200\n",
      "Epoch 2000, Loss: 0.0228\n",
      "Epoch 2100, Loss: 0.0223\n",
      "Epoch 2200, Loss: 0.0183\n",
      "Epoch 2300, Loss: 0.0191\n",
      "Epoch 2400, Loss: 0.0135\n",
      "Test Accuracy: 94.17%\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Implementation\n",
    "# Set learning rate, epochs, and batch size\n",
    "learning_rate = 0.01\n",
    "epochs = 2500\n",
    "batch_size = 32  # Set mini-batch size\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)# * 0.01\n",
    "biases_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)# * 0.01\n",
    "biases_output = np.zeros(output_size)\n",
    "\n",
    "# Run backpropagation with mini-batch gradient descent\n",
    "weights_input_hidden, biases_hidden, weights_hidden_output, biases_output = backPropagation_sigmoid(learning_rate, epochs, batch_size, X_train_scaled, y_train, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, \"sigmoid\")\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = calculate_accuracy(X_test_scaled, y_test, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, \"sigmoid\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0866\n",
      "Epoch 100, Loss: 0.8416\n",
      "Epoch 200, Loss: 0.4292\n",
      "Epoch 300, Loss: 0.0631\n",
      "Epoch 400, Loss: 0.0514\n",
      "Epoch 500, Loss: 0.0533\n",
      "Epoch 600, Loss: 0.0454\n",
      "Epoch 700, Loss: 0.0445\n",
      "Epoch 800, Loss: 0.0484\n",
      "Epoch 900, Loss: 0.0456\n",
      "Epoch 1000, Loss: 0.0372\n",
      "Epoch 1100, Loss: 0.0409\n",
      "Epoch 1200, Loss: 0.0405\n",
      "Epoch 1300, Loss: 0.0474\n",
      "Epoch 1400, Loss: 0.0403\n",
      "Epoch 1500, Loss: 0.0344\n",
      "Epoch 1600, Loss: 0.0336\n",
      "Epoch 1700, Loss: 0.0294\n",
      "Epoch 1800, Loss: 0.0447\n",
      "Epoch 1900, Loss: 0.0345\n",
      "Epoch 2000, Loss: 0.0315\n",
      "Epoch 2100, Loss: 0.0316\n",
      "Epoch 2200, Loss: 0.0316\n",
      "Epoch 2300, Loss: 0.0313\n",
      "Epoch 2400, Loss: 0.0375\n",
      "Test Accuracy: 92.50%\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Implementation\n",
    "# Set learning rate, epochs, and batch size\n",
    "learning_rate = 0.01\n",
    "epochs = 2500\n",
    "batch_size = 32  # Set mini-batch size\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)# * 0.01\n",
    "biases_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)# * 0.01\n",
    "biases_output = np.zeros(output_size)\n",
    "\n",
    "# Run backpropagation with mini-batch gradient descent\n",
    "weights_input_hidden, biases_hidden, weights_hidden_output, biases_output = backPropagation_sigmoid(learning_rate, epochs, batch_size, X_train_scaled, y_train, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, \"tanh\")\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = calculate_accuracy(X_test_scaled, y_test, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, \"tanh\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1080\n",
      "Epoch 100, Loss: 0.1000\n",
      "Epoch 200, Loss: 0.1000\n",
      "Epoch 300, Loss: 0.1000\n",
      "Epoch 400, Loss: 0.1000\n",
      "Epoch 500, Loss: 0.1000\n",
      "Epoch 600, Loss: 0.1000\n",
      "Epoch 700, Loss: 0.1000\n",
      "Epoch 800, Loss: 0.1000\n",
      "Epoch 900, Loss: 0.1000\n",
      "Epoch 1000, Loss: 0.1000\n",
      "Epoch 1100, Loss: 0.1000\n",
      "Epoch 1200, Loss: 0.1000\n",
      "Epoch 1300, Loss: 0.1000\n",
      "Epoch 1400, Loss: 0.1000\n",
      "Epoch 1500, Loss: 0.1000\n",
      "Epoch 1600, Loss: 0.1000\n",
      "Epoch 1700, Loss: 0.1000\n",
      "Epoch 1800, Loss: 0.1000\n",
      "Epoch 1900, Loss: 0.1000\n",
      "Epoch 2000, Loss: 0.1000\n",
      "Epoch 2100, Loss: 0.1000\n",
      "Epoch 2200, Loss: 0.1000\n",
      "Epoch 2300, Loss: 0.1000\n",
      "Epoch 2400, Loss: 0.1000\n",
      "Test Accuracy: 9.17%\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Implementation\n",
    "# Set learning rate, epochs, and batch size\n",
    "learning_rate = 0.01\n",
    "epochs = 2500\n",
    "batch_size = 32  # Set mini-batch size\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)# * 0.01\n",
    "biases_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)# * 0.01\n",
    "biases_output = np.zeros(output_size)\n",
    "\n",
    "# Run backpropagation with mini-batch gradient descent\n",
    "weights_input_hidden, biases_hidden, weights_hidden_output, biases_output = backPropagation_sigmoid(learning_rate, epochs, batch_size, X_train_scaled, y_train, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, \"relu\")\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = calculate_accuracy(X_test_scaled, y_test, weights_input_hidden, biases_hidden, weights_hidden_output, biases_output, \"relu\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best results given the hyperparemeters of learning_rate = 0.01, epochs = 2500, the best performing trial was using the sigmoid activation function with an accuracy value of 94.17%, \n",
    "#followed by tanh activation function with hyperparameters learning_rate = 0.01, epochs = 2500 with a result of 92.50% and finally the Relu activation function with a value of 9.17%\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "#Do predict\n",
    "#Find the index of the maximum value in the output later\n",
    "#Compare the index to the index in the one-hot encoded Y array\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
